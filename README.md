# Structure-Aware Semantic Chunking

[![Paper](https://img.shields.io/badge/Paper-Zenodo-green)](https://doi.org/10.5281/zenodo.17797912)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/)

**Don't let your RAG system break your tables.**

This repository contains the official implementation of the paper *"Structure-Aware Semantic Chunking: A Hybrid Penalty Mechanism for Document Segmentation"*.

### ğŸ’¡ The Problem
Traditional semantic chunking methods (like standard Max-Min) are "structure-blind". They merge distinct sections (headers, lists) if the semantic embedding is similar, destroying the logical layout of financial and legal documents (e.g., merging a "Implementation" header into a "Coding" paragraph).

### âœ¨ Our Solution
We introduce a lightweight **structure penalty term** into the clustering process. It forces segmentation at explicit boundaries (Headers, Markdown, Lists) regardless of semantic similarity.

### ğŸš€ Quick Start

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
2. **Generate the benchmark dataset**
   This script downloads technical articles from Wikipedia and injects structural traps (headers, lists) to create a "failure case" benchmark.
   ```bash
   cd scripts
   python generate_benchmark.py
   ```
   *Output: data/benchmark_50.json*
3. **Run the main evaluation**
   This script runs the comparison between the Baseline (Max-Min) and our Structure-Aware method on the 50-document corpus.
   ```bash
   python run_eval.py
   ```
   *Output: data/eval_summary.csv and LaTeX table code.*

4. **Run ablation studies**
   Verify the individual contribution of each rule (Headers vs. Lists vs. Full Method).
   ```bash
   python run_ablation.py
   ```
   *Output: Prints the incremental improvement of each structural rule.*
5. **Visualize results**
   Generate the similarity heatmaps and performance comparison charts used in the paper.
   ```bash
   python visualize_results.py
   ```
   Output: Images saved to ../images/

### ğŸ“Š Benchmark Results (N=50 Documents)
We evaluated the method on a diverse corpus of 50 technical documents across Law, Finance, Medicine, and CS. The results demonstrate a significant improvement in recovering ground-truth structure.
| Method | Average AMI Score | Improvement |
| :--- | :---: | :---: |
| Baseline (Semantic Only) | 0.2342 | - |
| **Structure-Aware (Ours)** | **0.9077** | **+0.6735** |

### ğŸ“‚ Dataset
The benchmark dataset constructed for this research is available in data/benchmark_50.json. It contains 50 structured documents with ground-truth section labels.

### ğŸ“‚ Project Structure

```text
Structure-Aware-RAG/
â”œâ”€â”€ data/                  # Generated benchmark datasets & evaluation logs
â”‚   â”œâ”€â”€ benchmark_50.json
â”‚   â””â”€â”€ eval_summary.csv
â”œâ”€â”€ images/                # Visualization outputs (heatmaps, plots)
â”œâ”€â”€ scripts/               # Experiment automation scripts
â”‚   â”œâ”€â”€ generate_benchmark.py  # Step 1: Data generation
â”‚   â”œâ”€â”€ run_eval.py            # Step 2: Main evaluation (Baseline vs Ours)
â”‚   â”œâ”€â”€ run_ablation.py        # Step 3: Ablation study
â”‚   â””â”€â”€ visualize_results.py   # Step 4: Plotting
â”œâ”€â”€ src/                   # Core algorithm package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ core.py            # StructureAwareChunker class implementation
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md
```

### ğŸ”— Citation
If you use this code or dataset, please cite our paper:
   ```bibtex

   @article{ye2025structure,
      title={Structure-Aware Semantic Chunking: A Hybrid Penalty Mechanism for Document Segmentation},
      author={Ruidian Ye},
      year={2025},
      publisher={Zenodo},
      doi={10.5281/zenodo.17797912}
   } 
   ```
### ğŸ“ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
